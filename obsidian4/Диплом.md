Целевая аудитория: разработчики на языке (?), которые используют github.

Аналоги:
+ **[Diffblue Cover](https://www.diffblue.com/resources/ai-powered-unit-testing-with-github-actions-the-basics/?utm_source=chatgpt.com)**  
	Этот инструмент использует ИИ для автоматической генерации юнит-тестов для Java-приложений. Он интегрируется с GitHub Actions, позволяя запускать тесты в CI/CD пайплайне. Diffblue Cover доступен в виде GitHub Action.
	Довольно хороший инструмент, он генерирует тесты посредством создания нового коммита в ПР. Имеет много настроек - гибкий.
+  [**Test Summary Action**](https://github.com/test-summary/action?utm_source=chatgpt.com) — Составьте удобный для чтения сводный отчет о тестовых данных вашего проекта в рамках рабочего процесса GitHub Actions CI/CD. Это поможет вам быстро понять, как изменения влияют на ваши запросы на вытягивание, и увидеть, какие изменения приводят к новым проблемам.
+ **[GitHub Test Reporter](https://github.com/ctrf-io/github-test-reporter?utm_source=chatgpt.com)**  
	Это приложение позволяет публиковать и просматривать отчеты о тестах непосредственно в вашем CI/CD пайплайне GitHub Actions и Pull Requests. Оно поддерживает все основные фреймворки тестирования и предоставляет подробные сводки, анализ неудачных тестов, обнаружение нестабильных тестов и ИИ-анализы.
	Довольно крутые отчеты + объяснение, почему тесты упали.
+ [Pynguin](https://github.com/se2p/pynguin) — первый инструмент, который заполняет этот пробел и позволяет автоматически генерировать модульные тесты для программ на Python. Проект обновляется. Pynguin — это приложение командной строки. После установки в виртуальную среду вы можете вызвать инструмент, введя команду `pynguin`внутри неё. После этого Pynguin выведет список параметров командной строки.
  
	**Pynguin выполняет тестируемый модуль!** В результате, в зависимости от того, какой код содержится в этом модуле, запуск Pynguin может нанести серьёзный вред вашему компьютеру, например, стереть весь жёсткий диск! Мы рекомендуем запускать Pynguin в изолированной среде; используйте, например, Docker-контейнер, чтобы минимизировать риск повреждения системы.
	
	**Pynguin — это всего лишь исследовательский прототип!** Он совершенно не предназначен для промышленного использования. Тем не менее, мы хотели бы увидеть Pynguin в стадии, готовой к эксплуатации. Пожалуйста, поделитесь с нами своим опытом использования Pynguin.

+ [ai-test-generator](https://github.com/attilagrynaeus/ai-test-generator) - Какой-то мини проект для генерации тестов на Java с помощью GPT-4. Запускается через терминал вызовом консольной команды.
+ [Qodo cover](https://github.com/qodo-ai/qodo-cover) Довольно неплохой проект, но перестал развиваться. Судя по его дорожной карте, есть что делать. Самый близкий аналог, который я нашел.
  Он создает отдельный пр с тестами и способен прогнать каждый тест самостоятельно. Он способен изменять тесты, если код функции был изменен.
1. **Test Runner:** выполняет команду или скрипты для запуска тестового набора и создания отчетов о покрытии кода.
2. **Анализатор покрытия:** проверяет, увеличивается ли покрытие кода по мере добавления тестов, гарантируя, что новые тесты вносят вклад в общую эффективность тестирования.
3. **Конструктор подсказок:** собирает необходимые данные из кодовой базы и создает подсказку для передачи в большую языковую модель (LLM).
4. **AI Caller:** взаимодействует с LLM для создания тестов на основе предоставленных подсказок.
![[Pasted image 20251005130028.png]]

+ [ai-test-reporter](https://github.com/ctrf-io/ai-test-reporter) - мощный инструмент, генерирующий интеллектуальные сводки результатов тестов с использованием широкого спектра моделей искусственного интеллекта. Благодаря доступу к более чем 300 моделям от различных поставщиков (OpenAI, Anthropic Claude, Google Gemini, Mistral, Perplexity, OpenRouter и других), он анализирует непройденные тесты и предоставляет полезную информацию о причинах ошибок и способах их устранения.
	Консольная утилита, можно интегрировать с GitHub Actions. Генерирует AI отчеты по тестам, их провалам и тд.
+ [Ai-test-generator](https://github.com/Parsavazifeh/AI-Test-Generator) - Тоже мини-проект. Генерирует тесты для питона с помощью GPT. По описание что-то неплохое, но почему-то 1 человек и 3 коммита 8 мес назад.
![[Pasted image 20251005142320.png]]

# Сценарий 

Тема: GitHub App для автоматической генерации тестов и CI‑анализ

+ Возможность использовать кастомный API AI ключ.
+ Дать возможность задавать свой промпт.
+ Язык, потенциально, может быть любой.
+ Наверное, можно добавить ограничения в настройках: минимальное покрытие, максимальное, максимальное время выполнение тестов, ограничение выполнения самого долгого теста.
+ Режим работы?.. Набор настроек, например, писать ли комментарий или нет. Может, насколько тесты детальны, проверять детально, тогда они будут больше и могут работать дольше.
+ Нужно помнить о связанных тестах.
+ Настройка, на что именно будет реагировать агент. Триггер.
+ Надо учесть уже готовые тесты, можно просить делать по аналогии с ними. Чтобы не анализировать весь проект, можно учесть только близкие к измененной области.
+ В какой момент нужно запустить и проверить тесты? Чтобы проверить работоспособность их и чтобы понять, что тесты, сгенерированные ИИ работают.
+ С каких запусков брать статистику для анализа и отчетов? Несколько запусков после генерации, если тесты падают -> менять.
+ Очевидно, тесты должны отличаться от уже существующих.
+ Что делать с тестами, которые устарели? Либо удалять ИИ, либо помечать и отдавать пользователя на ручное управление. Как вариант, вынести это в параметр.
+ Генерация документации по тестам? Или лишнее.
+ Задать триггер на отдельные файлы в настройках?
+ Дать возможно пользователям указывать для определённых путей определенные промпты.
+ Наверное, хочется иметь режим обратной связи разработчика и модели, чтобы разработчики могли как-то указать на корректность действий модели.
+ Может сделать режим обучение, где каждый тест будет подтверждаться пользователем, куда он может писать комментарии (если такое возможно) и уже по этим правкам будет немного форматироваться промпт. 
+ Нужно еще хорошо написать базовый промпт и выбрать подходящую модель!
+ Запуск тестов можно попробовать интегрировать с запуском CI гитовской, чтобы не поднимать свой докер или что-то такое. (в идеале)
+ Возможно, можно сделать процесс сканирования всего репозитория, который будет происходить раз в день 00:00, например. Он будет анализировать все тесты и выявлять лишние, дублирующие или совсем не нужные. Делать отчет и давать человеку сводку и рекомендации, при этом сам ничего не будет удалять/менять. Так можно будет поддерживать актуальность тестов.


	!!!!!!!!
	Не проще разве тогда просто использовать какого-нибудь ИИ агента, попросить его написать тесты и самому же просмотреть их, апрувнуть и тд. Если что-то не понравится, отклонить или задать уточнение сразу агенту?
	!!!!!!!!
+ Не нужно всегда помнить про их генерацию, один раз настроил и они будут создаваться автоматически.
+ Проверкой тестов придется занимать вручную. Pipeline сгенерировать тест через агента, чтобы потом ждать когда пройдет CI сомнительная идея. Наша программа автоматически должна это делать.
+ Отчеты по запускам не получится так просто сгенерить, так же нужно будет настраивать отдельную CI, например.


1) Система триггерится на какое событие, которое можно закастомить, по умолчанию на создание коммита. Или только на коммиты. 
2) Классификация изменений (новый функционал, рефакторинг, фикс бага)
3) Генерация тестов. Тут существует два момента, когда пользователь специально поменял функции и тесты перестали работать и, когда пользователь случайно сломал функцию. Такую ситуацию можно решить, первое, что пришло в голову, расставление специальных меток комментариев на функцию или весь файл целиком. Такая метка будет говорить ИИ, что для этой функции необходимо изменить тесты. Сами метки можно подчищать после генерации тестов и всех прогонов автоматически.
   Тесты будут генерировать только на основе новых изменений. 
4) Многоразовый запуск тестов.
5) Анализ этих тестов на покрытие, на потенциально долгие тесты, на какие-то возможные оптимизации. Возможно, покрытие отдельных элементов проекта как-то визуализировать.
6) Создание финального коммита (изменения пользователя + сгенерированные тесты).

Как я понял, не проверял, код программы просто будет отображаться как новый код пользователя или там нужно определенное подтверждение?


# Особенности, которых нет у других

+ Создание тестов по триггеру. В большинстве аналогов это нужно отдельно настраивать.
+ Умный анализ кода от AI. Много программ используют в своей основе алгоритмы построения деревьев связанности, пытаясь покрыть код по строкам, войти в каждый if, AI, в свою очередь, может поступать умнее. Лишняя часть тестов может быть отсеяна анализом AI. 
+ Простота и автоматизация. Часть проектов поддерживает github actions, а другая часть нет. Как по мне, в случае с actions работа выглядит более автоматизированной, 1 раз настроил и забыл.
+ "Всё в одном месте". Создание тестов + какой-то анализ - в большинстве случаев это две разные программы, которые настраиваются по отдельности.
+ Само наличие хоть какого-то анализа.
+ Старые тесты. Много аналогов, которые упускают это, они генерируют тесты только по новому коду.
+ У большинства аналогов отсутствует поддержка существующих тестов: они либо игнорируются, либо затираются. В нашем случае AI будет внимательнее подходить к измененным участкам кода (Если брать в работу фишку с флагами), анализировать по флагам необходимость изменять.
+ Если брать в расчет обычные AI агенты в IDE, то они не запускают тесты, а лишь генерируют их. в нагем случае будет происходить многоразовый запуск написанных тестов.
+ Гибка настройка в одном месте. В большинстве проектов, так как они являются консольными, все работает через флаги. Иметь один конфигурационный файл очень удобно. 
+ Хочется сделать работу с историей коммитов. *
  Как вижу это я: 
1. Пользователь устанавливает режим обучения. В этом режиме всякий тест требует подтверждения.
2. Модель запоминает, что пользователь принял, а что отклонил, ориентируясь на это, она меняет своей промпт.




## Резюме 06.10.25

Язык один пока взять.

Продемонстрировать работу пингвина. 
Посмотреть аналоги, которые встраиваются в CI. (топ-3)*

Понять, что является хорошей практикой или плохой в готовых решениях.
Про гибкость подробнее можно выяснить

Иметь подтветверждение что АСТ не эффективно по сравнию с ИИ

процесс обработки тестов не должен ломать готовый процесс разработки.

Подтверждение тестов. Настройки крутые.

Посмотреть как работают аналоги. Если есть много проблем в конфигурации, сделать акцент на это.

Если нельзя интегрировать в CI аналог, то не тратить на него время.

встроенные в CI + ИИ генерация обязательно

АСТ только для примера что их лучше не применять. Ссылочку почему АСТ хуже ИИ.

Если не делать его платным, то ок как преимущество.

Продумать архитектуру, накинуть какие-то схемы.